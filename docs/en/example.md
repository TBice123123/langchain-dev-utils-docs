# Usage Example

> This article demonstrates how to effectively use the `langchain-dev-utils` library in a `langchain` and `langgraph` project through a **multi-agent example**. This example deeply integrates all core modules of the library, helping you comprehensively master the practical usage of its core components.

## Process Description

This multi-agent architecture performs a complete analysis of a user-input topic theme. A main agent decomposes the topic into several subtopics and distributes them to multiple sub-agents for parallel processing. Each sub-agent is responsible for analyzing its assigned subtopic and recording the analysis results in note form. Subsequently, a large language model summarizes the analysis reports generated by all sub-agents to produce a final summary report.

The specific process is as follows:

- The user inputs a topic theme.
- The main agent (Dispatcher) decomposes the topic and distributes it to multiple sub-agents for parallel execution.
- Each sub-agent (Talker) performs its analysis task and writes the results to a note.
- Finally, a large language model (Summary) integrates the analysis content from all sub-agents to generate the final summary report.

## Project Setup

### Project Initialization

This project uses `uv` as the project management tool. First, install `uv`.

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

For Windows systems, use:

```bash
powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"
```

After installation, create the project using `uv`:

```bash
uv init langchain-dev-utils-example
```

Enter the project directory:

```bash
cd langchain-dev-utils-example
```

### Installing Dependencies

Ensure `python` is installed. If not, you can use `uv` to [install Python](https://docs.astral.sh/uv/guides/install-python/).

Install project dependencies:

```bash
uv add langchain langgraph langchain-dev-utils
```

Install `langgraph-cli` for debugging:

```bash
uv add langgraph-cli[inmem] --group dev
```

### Setting Up the Project Directory

Create a `src` directory in the project; all code will be placed here. Ensure the project directory structure is as follows:

```
langchain-dev-utils-example/
├── src/
│   ├── summary_agents/
│   │   ├── __init__.py
│   │   └── node.py
│   └── talker_agents/
│       ├── __init__.py
│       ├── graph.py
│       └── state.py
│   ├── __init__.py
│   ├── graph.py
│   ├── node.py
│   ├── prompt.py
│   ├── state.py
│   └── tools.py
├── .env
├── .gitignore
├── .python-version
├── langgraph.json
├── pyproject.toml
├── README.md
└── uv.lock
```

### Registering Model Providers

First, register the model providers. As recommended in [Model Management](./model-management.md), registration is typically completed in the project's `__init__.py` file to ensure initialization when the project starts.

This example uses four open-source models:

- `deepseek`: Integrated via `langchain-deepseek` (it's an officially `init_chat_model` supported provider, no need to register again).
- `qwen`: Integrated via `langchain-qwq` (needs registration, pass `ChatQwen` to `chat_model`).
- `kimi` and `glm`: Integrated via `langchain-openai` (need registration, but no suitable integration library exists; however, both providers support OpenAI-style APIs, so use `langchain-openai` for access, pass `openai` to `chat_model`).

Register the model providers in `src/__init__.py`:

```python
from langchain_dev_utils import batch_register_model_provider
from langchain_qwq import ChatQwen

batch_register_model_provider(
    [
        {"provider": "dashscope", "chat_model": ChatQwen},
        {
            "provider": "zai",
            "chat_model": "openai",
            "base_url": "https://open.bigmodel.cn/api/paas/v4/",
        },
        {
            "provider": "moonshot",
            "chat_model": "openai",
            "base_url": "https://api.moonshot.cn/v1",
        },
    ]
)
```

You can test if the registration was successful with the following code:

```python
from langchain_dev_utils import load_chat_model

model = load_chat_model("dashscope:qwen3-235b-a22b-instruct-2507")
print(model.invoke("hello"))
```

If this runs successfully, the model provider registration was successful.

## Writing the Main Agent

### Writing Tools

The main agent includes four tools:

- `ls`: Lists existing note names.
- `query_note`: Queries the specific content of a note.
- `transfor_to_talk`: A routing agent that transfers the task to multiple agents for discussion based on user needs.
- `ask_human_for_more_details`: Asks the user for more details about the topic.

Use the utility functions provided by [Context Engineering](./context-engineering.md) to create the first two tools:

```python
from langchain_dev_utils import create_ls_tool, create_query_note_tool

ls = create_ls_tool(
    name="ls",
    description="""Used to list all saved note names.

    Returns:
    list[str]: A list containing all note filenames.
    """,
)

query_note = create_query_note_tool(
    name="query_note",
    description="""Used to query a note.

    Parameters:
    file_name: The note name.

    Returns:
    str, The content of the queried note.
    """,
)
```

The `transfor_to_talk` tool is implemented as follows:

```python
from langchain.tools import tool
from typing import Annotated

@tool
async def transfor_to_talk(
    sub_topics: Annotated[
        list[str],
        "List of subtopics for the current discussion topic.",
    ],
):
    """Used to transfer the topic to sub-agents for discussion."""

    return "transfor success!"
```

The `ask_human_for_more_details` tool implementation:

This implementation uses the `human_in_the_loop_async` decorator from [Tool Enhancement](./tool-enhancement.md).

```python
from langchain_dev_utils import human_in_the_loop_async, InterruptParams
from langgraph import interrupt
from typing import Any

async def handler(params: InterruptParams) -> Any:
    response = interrupt(
        f"I have some questions about this topic: {params['tool_call_args'].get('question')}."
    )
    return response["answer"]


@human_in_the_loop_async(handler=handler)
async def ask_human_for_more_details(
    question: Annotated[str, "Questions about the current discussion topic."],
):
    """Used to get more details from the user about the current discussion topic."""
    return "ask human for more details"
```

**Code Location: `src/tools.py`**

### Writing the State

Main agent state schema:

```python
from typing import Annotated, TypedDict

from langchain_core.messages import AnyMessage
from langchain_dev_utils import NoteStateMixin
from langgraph.graph import add_messages


class State(NoteStateMixin):
    talker_list: list[str]
    topic: str
    dispatcher_messages: Annotated[list[AnyMessage], add_messages]


class StateIn(TypedDict):
    talker_list: list[str]
    topic: str


class StateOut(NoteStateMixin):
    pass
```

**Code Location: `src/state.py`**

State key definitions:

- `topic`: Stores the user-input topic theme.
- `talker_list`: Stores the agents participating in the discussion.
- `dispatcher_messages`: Stores the main agent's conversation messages.
- `note`: Stores notes (inherited from `NoteStateMixin`).

### Writing the Node

```python
from typing import Literal, cast
from langchain_core.messages import AIMessage, SystemMessage
from langgraph.prebuilt.tool_node import ToolNode
from langgraph.types import Command
from src.state import State
from src.tools import ask_human_for_more_details, transfor_to_talk, ls, query_note
from langchain_dev_utils import (
    has_tool_calling,
    load_chat_model,
    parse_tool_calling,
)
from src.prompt import DISPATCHER_PROMPT


async def dispatcher(
    state: State,
) -> Command[Literal["__end__", "talker", "dispatcher_tools"]]:
    model = load_chat_model("deepseek:deepseek-chat")
    bind_model = model.bind_tools(
        [transfor_to_talk, ls, query_note, ask_human_for_more_details]
    )

    response = await bind_model.ainvoke(
        [
            SystemMessage(
                content=DISPATCHER_PROMPT.format(
                    topic=state["topic"],
                    num=len(state["talker_list"]),
                )
            ),
            *state["dispatcher_messages"],
        ]
    )

    if has_tool_calling(cast(AIMessage, response)):
        tool_call_name, _ = parse_tool_calling(
            cast(AIMessage, response), first_tool_call_only=True
        )

        if tool_call_name == "transfor_to_talk":
            return Command(
                goto="talker",
                update={
                    "dispatcher_messages": [response],
                },
            )

        return Command(
            goto="dispatcher_tools", update={"dispatcher_messages": [response]}
        )
    return Command(goto="__end__", update={"dispatcher_messages": [response]})


dispatcher_tools = ToolNode(
    [ls, query_note, ask_human_for_more_details], messages_key="dispatcher_messages"
)
```

**Code Location: `src/node.py`**

Using `has_tool_calling` and `parse_tool_calling` functions from [Message Processing](./message-processing.md):

- `has_tool_calling`: Determines if a message contains a tool call.
- `parse_tool_calling`: Parses tool calls, returning a list of `(name, args)` tuples.

### Writing the Prompt

Main agent prompt:

```markdown
You are a coordinator and problem decomposer for topic discussions.
Your task is very clear:

Use the `transfor_to_talk` tool to decompose the topic "{topic}" into {num} subtopics and write the discussion results for each subtopic into notes.
After this tool executes, it will directly return a result, which is the final summary report of the several subtopics.

If you need to ask the user for more details about the topic, you can use the `ask_human_for_more_details` tool.

Additional Notes:

You have permission to use the `query_note` and `ls` tools. If you want to emphasize the discussion results of a certain subtopic, you can use the `query_note` and `ls` tools.
```

## Discussion Agents

### Writing Tools

The discussion agent needs the `tavily_search` tool for internet searches.

Install the dependency:

```bash
uv add langchain-tavily
```

The corresponding tool implementation:

```python
from langchain_community.tools.tavily_search import TavilySearch
from langchain.tools import tool
from typing import Annotated

@tool
async def tavily_search(query: Annotated[str, "Content to search for"]):
    """Internet search tool, used to obtain the latest online information and materials. Note: To control context length and reduce invocation costs, this tool can only be called once per task execution."""
    tavily_search = TavilySearch(max_results=5)
    result = await tavily_search.ainvoke({"query": query})
    return result
```

**Code Location: `src/tools.py`**

**Note: The `TAVILY_API_KEY` environment variable must be set.**

### Writing the State

```python
from langchain_core.messages import AnyMessage
from langgraph.graph import MessagesState, add_messages
from typing import Annotated
from langchain_dev_utils import NoteStateMixin


class TalkState(MessagesState, NoteStateMixin):
    talker_list: list[str]
    sub_topic: Annotated[str, lambda x, y: y] # Holds the subtopic for the current agent
    dispatcher_messages: Annotated[list[AnyMessage], add_messages]
    remaining_steps: int
```

**Code Location: `src/talker_agents/state.py`**

State key definitions:

- `sub_topic`: The subtopic that the current sub-agent needs to analyze.
- `talker_list`: Stores the discussion participants.
- `dispatcher_messages`: Stores the main agent's messages.
- `remaining_steps`: A required key for the prebuilt agent.
- `note`: Stores notes (inherited from `NoteStateMixin`).
- `messages`: The context window for the sub-agent's execution.

### Writing the Graph

Each discussion agent is built using the [Prebuilt Agent](./prebuilt.md).

```python
from typing import Any, cast
from langchain_core.messages import AIMessage, HumanMessage
from langchain_dev_utils import (
    has_tool_calling,
    parallel_pipeline,
    parse_tool_calling,
)
from langchain_dev_utils.prebuilt import create_agent
from langgraph.types import Send
from src.prompt import TALK_PROMPT
from src.talker_agents.state import TalkState
from src.tools import tavily_search, write_note

talk_name_map = {
    "qwen": "dashscope:qwen3-235b-a22b-instruct-2507",
    "kimi": "moonshot:kimi-k2-0905-preview",
    "glm": "zai:glm-4.5",
}


def branch_talker(state: TalkState):
    message = state["dispatcher_messages"][-1]
    if has_tool_calling(message=cast(AIMessage, message)):
        _, args = parse_tool_calling(
            cast(AIMessage, message), first_tool_call_only=True
        )

        sub_topics = cast(dict[str, Any], args).get("sub_topics", "")

        return [
            Send(
                node=talk_name,
                arg={
                    "sub_topic": sub_topic,
                },
            )
            for sub_topic, talk_name in zip(
                sub_topics,
                state["talker_list"] if "talker_list" in state else ["kimi", "qwen"],
            )
        ]

    return [Send(node="__end__", arg={})]


def dynamic_prompt(state: TalkState):
    messages = state["messages"]
    return [
        HumanMessage(content=TALK_PROMPT.format(topic=state["sub_topic"])),
        *messages,
    ]


talkers = parallel_pipeline(
    [
        create_agent(
            model=talker_model,
            tools=[tavily_search, write_note],
            state_schema=TalkState,
            prompt=dynamic_prompt,
            name=talker_name,
        )
        for talker_name, talker_model in talk_name_map.items()
    ],
    state_schema=TalkState,
    branches_fn=branch_talker,
    graph_name="talk",
)
```

Then, for these sub-agents, use [State Graph Orchestration](./graph-orchestration.md) to build a parallel agent pipeline, using `branches_fn` to allow the user to specify which agents participate in the discussion.

### Writing the Prompt

Discussion agent prompt:

```markdown
Your task is to discuss based on the user's topic. You can use the `tavily_search` tool for internet searches.
The user's topic is {topic}.

After completion, you must use the `write_note` tool to write the final result to a note.
```

## Summary Node

### Writing the Node

The code implementation is as follows:

```python
from langchain_dev_utils import load_chat_model, message_format
from langchain_core.messages import AIMessage, SystemMessage, ToolMessage
from typing import cast
from src.state import State
from src.prompt import SUMMARY_PROMPT


async def summary_node(
    state: State,
):
    model = load_chat_model("dashscope:qwen3-235b-a22b-instruct-2507")
    response = await model.ainvoke(
        [
            SystemMessage(
                content=SUMMARY_PROMPT.format(
                    result=message_format(
                        [state["note"][note_name] for note_name in state["note"]],
                        separator="-" * 10 + "\n",
                    )
                )
            )
        ]
    )

    tool_call_id = cast(AIMessage, state["dispatcher_messages"][-1]).tool_calls[0]["id"]

    return {
        "dispatcher_messages": [
            ToolMessage(
                content=f"Discussion Summary: {response.content}", tool_call_id=tool_call_id
            )
        ]
    }
```

**Code Location: `src/summary_agent/node.py`**

## Writing the Prompt

```markdown
Your task is to summarize the content discussed by multiple agents.

The discussion results from each agent are:
{result}
```

Here, the `message_format` function from [Message Processing](./message-processing.md) is used to format a list of strings into a single string.

## Final Graph Construction

```python
from src.node import (
    dispatcher,
    dispatcher_tools,
)
from langgraph.graph import StateGraph
from src.state import State, StateIn, StateOut
from src.talker_agents.graph import talkers
from src.summary_agents.node import summary_node


graph = StateGraph(State, input_schema=StateIn, output_schema=StateOut)
graph.add_node("dispatcher", dispatcher)
graph.add_node("dispatcher_tools", dispatcher_tools)


graph.add_node("talker", talkers)
graph.add_node("summary", summary_node)
graph.add_edge("__start__", "dispatcher")
graph.add_edge("dispatcher_tools", "dispatcher")
graph.add_edge("talker", "summary")
graph.add_edge("summary", "dispatcher")


graph = graph.compile()
```

**Code Location: `src/graph.py`**

Create `langgraph.json` in the project root directory:

```json
{
  "dependencies": ".",
  "graphs": {
    "graph": "./src/graph.py:graph"
  }
}
```

Run `LangGraph Studio`:

```bash
langgraph dev
```

The final graph structure is as follows:

![Final Graph](/img/graph.png)
