# Usage Example

> This article demonstrates how to efficiently use the `langchain-dev-utils` library in `langchain` and `langgraph` projects through a **multi-agent example**. This example deeply integrates all core modules of this library, helping you comprehensively master the practical usage of the core modules.

## Process Description

This multi-agent architecture performs a complete analysis of a user-input topic theme. The main agent decomposes the topic into several subtopics and distributes them to multiple sub-agents for parallel processing. Each sub-agent is responsible for analyzing its assigned subtopic and recording the analysis results in note form. Subsequently, an LLM summarizes the analysis reports generated by all sub-agents to form a final summary report.

The specific process is as follows:

- The user inputs a topic theme.
- The main agent (Dispatcher) decomposes the topic and distributes it to multiple sub-agents for parallel execution.
- Each sub-agent (Talker) performs its analysis task and writes the analysis results into notes.
- Finally, an LLM (Summary) integrates the analysis content from all sub-agents to generate the final summary report.

## Project Setup

### Project Initialization

This project uses `uv` as the project management tool. First, you need to install `uv`.

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

For Windows systems, you can use:

```bash
powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"
```

After installation, use `uv` to create the project:

```bash
uv init langchain-dev-utils-example
```

Enter the project directory:

```bash
cd langchain-dev-utils-example
```

### Installing Dependencies

Ensure `python` is installed. If not, you can use `uv` to [install Python](https://docs.astral.sh/uv/guides/install-python/).

Install project dependencies:

```bash
uv add langchain langgraph langchain-dev-utils
```

Install `langgraph-cli` for debugging:

```bash
uv add langgraph-cli[inmem] --group dev
```

### Setting Up the Project Directory

Create a `src` directory in the project, where all code will be placed. Ensure the project directory structure is as follows:

```
langchain-dev-utils-example/
├── src/
│   ├── summary_agents/
│   │   ├── __init__.py
│   │   └── node.py
│   └── talker_agents/
│       ├── __init__.py
│       ├── agent.py
│       ├── graph.py
│       └── state.py
│   ├── __init__.py
│   ├── graph.py
│   ├── node.py
│   ├── prompt.py
│   ├── state.py
│   └── tools.py
├── .env
├── .gitignore
├── .python-version
├── langgraph.json
├── pyproject.toml
├── README.md
└── uv.lock
```

### Registering Model Providers

First, register the model providers. According to the content in [Model Management](./model-management.md), we need to complete the registration of model providers when the project starts. Therefore, consider writing the registration code in the project's `__init__.py` file to ensure registration is completed upon project startup.

This example uses four open-source models:
`deepseek`, `qwen`, `kimi`, and `glm`.
All four models provide OpenAI-compatible APIs, so they can be integrated directly using chat_model as `openai-compatible`. Note, however, that since deepseek is a model provider officially supported by `init_chat_model`, it does not need to be registered again.

Register model providers in `src/__init__.py`:

```python
from langchain_dev_utils.chat_models import batch_register_model_provider
from dotenv import load_dotenv

load_dotenv()

batch_register_model_provider(
    [
        {
            "provider": "dashscope",
            "chat_model": "openai-compatible",
        },
        {
            "provider": "zai",
            "chat_model": "openai-compatible",
        },
        {
            "provider": "moonshot",
            "chat_model": "openai-compatible",
        },
    ]
)

```

**Note**: The above code does not pass `base_url` because it is configured in the `.env` file. Therefore, you need to configure `DASHSCOPE_API_BASE`, `ZAI_API_BASE`, and `MOONSHOT_API_BASE` in the `.env` file. Also, configure `DASHSCOPE_API_KEY`, `ZAI_API_KEY`, and `MOONSHOT_API_KEY`.
You can test if the registration is successful with the following code:

```python
from langchain_dev_utils.chat_models import load_chat_model

model = load_chat_model("dashscope:qwen3-235b-a22b-instruct-2507")
print(model.invoke("hello"))
```

If this runs successfully, the model provider registration was successful.

## Main Agent Development

### Tools Development

The main agent includes four tools:

- `ls`: Lists existing notes
- `query_file`: Queries the specific content of a note
- `transfor_to_talk`: Routing agent, transfers tasks to multiple agents for discussion based on user needs
- `ask_human_for_more_details`: Requests more topic details from the user

Use the tool functions provided in [Agent Development](./agent-development.md) to create the first two tools:

```python
from langchain_dev_utils.agents.file_system import (
    create_ls_file_tool,
    create_query_file_tool
)


ls = create_ls_file_tool(
    name="ls",
    description="""Used to list the names of all saved notes.

    Returns:
    list[str]: A list containing all note filenames

    """,
)

query_file = create_query_file_tool(
    name="query_file",
    description="""Used to query files.

    Parameters:
    file_name: The file name

    Returns:
    str, The content of the queried file

    """,
)

```

The `transfor_to_talk` tool is implemented as follows:

```python
from langchain.tools import tool
from typing import Annotated

@tool
async def transfor_to_talk(
    sub_topics: Annotated[
        list[str],
        "List of subtopics for the current discussion topic",
    ],
):
    """Used to transfer the topic to sub-agents for discussion"""

    return "transfor success!"
```

The `ask_human_for_more_details` tool implementation:

You can use the `interrupt` function from `langgraph` to implement interrupting and asking the user. You can use this function directly or refer to the implementation below.
This implementation uses the `human_in_the_loop_async` decorator from [Tool Calling](./tool-calling.md).

```python
from langchain_dev_utils.tool_calling import InterruptParams, human_in_the_loop_async

async def handler(params: InterruptParams) -> Any:
    response = interrupt(
        f"I have some questions about this topic: {params['tool_call_args'].get('question')}."
    )
    return response["answer"]


@human_in_the_loop_async(handler=handler)
async def ask_human_for_more_details(
    question: Annotated[str, "Questions about the current discussion topic"],
):
    """Used to get more details from the user about the current discussion topic"""
    return "ask human for more details"

```

**Code location: `src/tools.py`**

### State Development

Main agent state Schema:

```python
from typing import Annotated, TypedDict

from langchain_core.messages import AnyMessage
from langchain_dev_utils.agents.file_system import FileStateMixin
from langgraph.graph import add_messages


class State(FileStateMixin):
    talker_list: list[str]
    topic: str
    dispatcher_messages: Annotated[list[AnyMessage], add_messages]


class StateIn(TypedDict):
    talker_list: list[str]
    topic: str


class StateOut(FileStateMixin):
    pass
```

**Code location: `src/state.py`**

State key definitions:

- `topic`: Stores the user-input topic theme
- `talker_list`: Stores the agents participating in the discussion
- `dispatcher_messages`: Stores the main agent's conversation messages
- `file`: Stores files (inherits from `FileStateMixin`)

### Node Development

```python
from typing import Literal, cast
from langchain_core.messages import AIMessage, SystemMessage
from langgraph.prebuilt.tool_node import ToolNode
from langgraph.types import Command
from src.state import State
from src.tools import ask_human_for_more_details, transfor_to_talk, ls, query_file
from langchain_dev_utils.tool_calling import has_tool_calling, parse_tool_calling
from langchain_dev_utils.chat_models import load_chat_model
from src.prompt import DISPATCHER_PROMPT


async def dispatcher(
    state: State,
) -> Command[Literal["__end__", "talker", "dispatcher_tools"]]:
    model = load_chat_model("deepseek:deepseek-chat")
    bind_model = model.bind_tools(
        [transfor_to_talk, ls, query_file, ask_human_for_more_details]
    )

    response = await bind_model.ainvoke(
        [
            SystemMessage(
                content=DISPATCHER_PROMPT.format(
                    topic=state["topic"],
                    num=len(state["talker_list"]),
                )
            ),
            *state["dispatcher_messages"],
        ]
    )

    if has_tool_calling(cast(AIMessage, response)):
        tool_call_name, _ = parse_tool_calling(
            cast(AIMessage, response), first_tool_call_only=True
        )

        if tool_call_name == "transfor_to_talk":
            return Command(
                goto="talker",
                update={
                    "dispatcher_messages": [response],
                },
            )

        return Command(
            goto="dispatcher_tools", update={"dispatcher_messages": [response]}
        )
    return Command(goto="__end__", update={"dispatcher_messages": [response]})


dispatcher_tools = ToolNode(
    [ls, query_file, ask_human_for_more_details], messages_key="dispatcher_messages"
)
```

**Code location: `src/node.py`**

Using `has_tool_calling` and `parse_tool_calling` functions from [Tool Calling](./tool-calling.md):

- `has_tool_calling`: Determines if a message contains tool calls
- `parse_tool_calling`: Parses tool calls, returns a list of `(name, args)` tuples

### Prompt Development

Main agent prompt:

```markdown
You are a coordinator and problem decomposer for topic discussions.
Your task is very clear:

Use the `transfor_to_talk` tool to decompose the topic "{topic}" into {num} subtopics and write the discussion results for each subtopic into notes.
After this tool executes, it will directly return a result, which is the final summary report of the several subtopics.

If you need to ask the user for more details about the topic, you can use the `ask_human_for_more_details` tool.

Additional Notes:

You have permission to use the `query_file` and `ls` tools. If you want to emphasize the discussion results of a certain subtopic, you can use the `query_file` and `ls` tools.
```

## Discussion Agents

### Tools Development

The discussion agent requires the `tavily_search` tool for internet searches.

Install dependency:

```bash
uv add tavily-python
```

Corresponding tool implementation:

```python
from tavily import AsyncTavilyClient
async def tavily_search(query: Annotated[str, "Content to search for"]):
    """Internet search tool, used to obtain the latest online information and data. Note: To control context length and reduce invocation costs, this tool can only be called once during the execution of each task."""
    tavily_search = AsyncTavilyClient(
        api_key=os.getenv("TAVILY_API_KEY"),
    )
    result = await tavily_search.search(query, max_results=3)
    return result
```

Additionally, there is a tool for writing files:

```python
from langchain_dev_utils.agents.file_system import create_write_file_tool
write_file = create_write_file_tool(
    name="write_file",
    description="""Tool used to write files.

    Parameters:
    content: str, File content

    """,
)
```

**Code location: `src/tools.py`**

**Note: The `TAVILY_API_KEY` environment variable must be set**

### State Development

```python
from langchain_core.messages import AnyMessage
from langgraph.graph import add_messages
from typing import Annotated
from langchain_dev_utils.agents.file_system import FileStateMixin
from langchain.agents import AgentState


class TalkState(AgentState, FileStateMixin):
    talker_list: list[str]
    sub_topic: Annotated[str, lambda x, y: y]
    dispatcher_messages: Annotated[list[AnyMessage], add_messages]
```

**Code location: `src/talker_agents/state.py`**

State key definitions:

- `sub_topic`: The subtopic that the current sub-agent needs to analyze
- `talker_list`: Stores discussion participants
- `dispatcher_messages`: Stores main agent messages
- `messages`: The context window for sub-agent execution

### Graph Development

#### Talker agent development

```python
from typing import Literal
from langchain_dev_utils.chat_models import load_chat_model
from langchain_dev_utils.tool_calling import has_tool_calling
from langgraph.graph import StateGraph
from langgraph.prebuilt.tool_node import ToolNode
from langgraph.types import Command
from src.prompt import TALK_PROMPT
from src.tools import tavily_search, write_file
from src.talker_agents.state import TalkState
from langchain_core.messages import SystemMessage


def create_talker_agent(model_name: str, agent_name: str):
    model = load_chat_model(model_name).bind_tools([tavily_search, write_file])

    async def call_model(
        state: TalkState,
    ) -> Command[Literal["__end__", "talker_tool_node"]]:
        response = await model.ainvoke(
            [
                SystemMessage(
                    content=TALK_PROMPT.format(
                        topic=state["sub_topic"],
                    )
                ),
                *state["messages"],
            ]
        )
        if has_tool_calling(response):
            return Command(
                goto="talker_tool_node",
                update={
                    "messages": [response],
                },
            )

        return Command(goto="__end__", update={"messages": [response]})

    graph = StateGraph(TalkState)
    graph.add_node("call_model", call_model)
    graph.add_node("talker_tool_node", ToolNode([tavily_search, write_file]))
    graph.add_edge("__start__", "call_model")
    graph.add_edge("call_model", "__end__")
    graph.add_edge("talker_tool_node", "call_model")
    graph = graph.compile(
        name=agent_name,
    )
    return graph
```

**Code location: `src/talker_agents/agent.py`**

### Parallel Composition of Talker Agents

```python
from src.talker_agents.state import TalkState

talk_name_map = {
    "qwen": "dashscope:qwen3-235b-a22b-instruct-2507",
    "kimi": "moonshot:kimi-k2-0905-preview",
    "glm": "zai:glm-4.5",
    "deepseek": "deepseek:deepseek-chat",
}


def branch_talker(state: TalkState):
    message = state["dispatcher_messages"][-1]
    if has_tool_calling(message=cast(AIMessage, message)):
        _, args = parse_tool_calling(
            cast(AIMessage, message), first_tool_call_only=True
        )

        sub_topics = cast(dict[str, Any], args).get("sub_topics", "")

        return [
            Send(
                node=talk_name,
                arg={
                    "sub_topic": sub_topic,
                    "messages": [HumanMessage(content="请帮我讨论这个话题")],
                },
            )
            for sub_topic, talk_name in zip(
                sub_topics,
                state["talker_list"] if "talker_list" in state else ["kimi", "qwen"],
            )
        ]

    return [Send(node="__end__", arg={})]


talkers = parallel_pipeline(
    [
        create_talker_agent(talker_model, talker_name)
        for talker_name, talker_model in talk_name_map.items()
    ],
    state_schema=TalkState,
    branches_fn=branch_talker,
    graph_name="talk",
)
```

Then, for these sub-agents, use the [State Graph Orchestration](./graph-orchestration.md) to build a parallel agent pipeline, implementing user-specified participating discussion agents through `branches_fn`.

### Prompt Development

Discussion agent prompt:

```markdown
Your task is to discuss based on the user's topic. You can use the `tavily_search` tool for internet searches.
The user's topic is {topic}

After completion, you must use the `write_file` tool to write the final result into a note.
```

## Summary Node

### Node Development

The code implementation is as follows:

```python
from langchain_dev_utils.chat_models import load_chat_model
from langchain_dev_utils.message_convert import format_sequence
from langchain_core.messages import AIMessage, SystemMessage, ToolMessage
from typing import cast
from src.state import State
from src.prompt import SUMMARY_PROMPT


async def summary_node(
    state: State,
):
    model = load_chat_model("dashscope:qwen3-235b-a22b-instruct-2507")
    response = await model.ainvoke(
        [
            SystemMessage(
                content=SUMMARY_PROMPT.format(
                    result=format_sequence(
                        [state["file"][file_name] for file_name in state["file"]],
                        separator="-" * 10 + "\n",
                    )
                )
            )
        ]
    )

    tool_call_id = cast(AIMessage, state["dispatcher_messages"][-1]).tool_calls[0]["id"]

    return {
        "dispatcher_messages": [
            ToolMessage(
                content=f"Discussion Summary:{response.content}", tool_call_id=tool_call_id
            )
        ]
    }
```

**Code location: `src/summary_agent/node.py`**

### Prompt Development

```markdown
Your task is to summarize the content discussed by multiple agents.

The discussion results of each agent are:
{result}
```

Here, the `format_sequence` function from [Message Processing](./message-conversion.md) is used to format a list of strings into a single string.

## Final Graph Construction

```python
from src.node import (
    dispatcher,
    dispatcher_tools,
)
from langgraph.graph import StateGraph
from src.state import State, StateIn, StateOut
from src.talker_agents.graph import talkers
from src.summary_agents.node import summary_node


graph = StateGraph(State, input_schema=StateIn, output_schema=StateOut)
graph.add_node("dispatcher", dispatcher)
graph.add_node("dispatcher_tools", dispatcher_tools)


graph.add_node("talker", talkers)
graph.add_node("summary", summary_node)
graph.add_edge("__start__", "dispatcher")
graph.add_edge("dispatcher_tools", "dispatcher")
graph.add_edge("talker", "summary")
graph.add_edge("summary", "dispatcher")


graph = graph.compile()
```

**Code location: `src/graph.py`**

Create `langgraph.json` in the project root directory:

```json
{
  "dependencies": ".",
  "graphs": {
    "graph": "./src/graph.py:graph"
  }
}
```

Run `LangGraph Studio`:

```bash
langgraph dev
```

The final graph structure is as follows:

![Final Graph](/img/graph.png)
