# çŠ¶æ€å›¾ç¼–æ’ç®¡é“

> [!NOTE]
>
> **åŠŸèƒ½æ¦‚è¿°**ï¼šä¸»è¦ç”¨äºå®ç°å¤šä¸ªçŠ¶æ€å›¾çš„å¹¶è¡Œå’Œä¸²è¡Œç»„åˆã€‚
>
> **å‰ç½®è¦æ±‚**ï¼šäº†è§£ langchain çš„[å­å›¾](https://docs.langchain.com/oss/python/langgraph/use-subgraphs),[Send](https://docs.langchain.com/oss/python/langgraph/graph-api#send)ã€‚
>
> **é¢„è®¡é˜…è¯»æ—¶é—´**ï¼š5 åˆ†é’Ÿ

## ä¸²è¡Œç®¡é“

å°†çŠ¶æ€å›¾æŒ‰ç…§é¡ºåºä¾æ¬¡è¿æ¥ï¼Œå½¢æˆä¸²è¡Œæ‰§è¡Œæµç¨‹ã€‚

é€šè¿‡ä¸‹é¢å‡½æ•°å®ç°:

- `sequential_pipeline` - ä»¥ä¸²è¡Œæ–¹å¼ç»„åˆå¤šä¸ªçŠ¶æ€å›¾

æ”¯æŒçš„å‚æ•°å¦‚ä¸‹:

- `sub_graphs`: è¦ç»„åˆçš„çŠ¶æ€å›¾åˆ—è¡¨ï¼ˆå¿…é¡»æ˜¯ StateGraph å®ä¾‹ï¼‰
- `state_schema`: æœ€ç»ˆç”Ÿæˆå›¾çš„ State Schema
- `graph_name`: æœ€ç»ˆç”Ÿæˆå›¾çš„åç§°ï¼ˆå¯é€‰ï¼‰
- `context_schema`: æœ€ç»ˆç”Ÿæˆå›¾çš„ Context Schemaï¼ˆå¯é€‰ï¼‰
- `input_schema`: æœ€ç»ˆç”Ÿæˆå›¾çš„è¾“å…¥ Schemaï¼ˆå¯é€‰ï¼‰
- `output_schema`: æœ€ç»ˆç”Ÿæˆå›¾çš„è¾“å‡º Schemaï¼ˆå¯é€‰ï¼‰
- `checkpoint`: LangGraph çš„æŒä¹…åŒ– Checkpointï¼ˆå¯é€‰ï¼‰
- `store`: LangGraph çš„æŒä¹…åŒ– Storeï¼ˆå¯é€‰ï¼‰
- `cache`: LangGraph çš„ Cacheï¼ˆå¯é€‰ï¼‰

ä½¿ç”¨ç¤ºä¾‹:

```python
import datetime
from langchain.agents import AgentState
from langchain_core.messages import HumanMessage
from langchain_dev_utils.agents import create_agent
from langchain_dev_utils.pipeline import sequential_pipeline
from langchain_core.tools import tool
from langchain_dev_utils.chat_models import register_model_provider

register_model_provider(
    provider_name="vllm",
    chat_model="openai-compatible",
    base_url="http://localhost:8000/v1",
)

@tool
def get_current_time():
    """è·å–å½“å‰æ—¶é—´"""
    return datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")


@tool
def get_current_weather():
    """è·å–å½“å‰å¤©æ°”"""
    return "æ™´å¤©"


@tool
def get_current_user():
    """è·å–å½“å‰ç”¨æˆ·"""
    return "å¼ ä¸‰"


# æ„å»ºé¡ºåºç®¡é“ï¼ˆæ‰€æœ‰å­å›¾é¡ºåºæ‰§è¡Œï¼‰
graph = sequential_pipeline(
    sub_graphs=[
        create_agent(
            model="vllm:qwen3-4b",
            tools=[get_current_time],
            system_prompt="ä½ æ˜¯ä¸€ä¸ªæ—¶é—´æŸ¥è¯¢åŠ©æ‰‹,ä»…èƒ½å›ç­”å½“å‰æ—¶é—´,å¦‚æœè¿™ä¸ªé—®é¢˜å’Œæ—¶é—´æ— å…³,è¯·ç›´æ¥å›ç­”æˆ‘æ— æ³•å›ç­”",
            name="time_agent",
        ),
        create_agent(
            model="vllm:qwen3-4b",
            tools=[get_current_weather],
            system_prompt="ä½ æ˜¯ä¸€ä¸ªå¤©æ°”æŸ¥è¯¢åŠ©æ‰‹,ä»…èƒ½å›ç­”å½“å‰å¤©æ°”,å¦‚æœè¿™ä¸ªé—®é¢˜å’Œå¤©æ°”æ— å…³,è¯·ç›´æ¥å›ç­”æˆ‘æ— æ³•å›ç­”",
            name="weather_agent",
        ),
        create_agent(
            model="vllm:qwen3-4b",
            tools=[get_current_user],
            system_prompt="ä½ æ˜¯ä¸€ä¸ªç”¨æˆ·æŸ¥è¯¢åŠ©æ‰‹,ä»…èƒ½å›ç­”å½“å‰ç”¨æˆ·,å¦‚æœè¿™ä¸ªé—®é¢˜å’Œç”¨æˆ·æ— å…³,è¯·ç›´æ¥å›ç­”æˆ‘æ— æ³•å›ç­”",
            name="user_agent",
        ),
    ],
    state_schema=AgentState,
)

response = graph.invoke({"messages": [HumanMessage("ä½ å¥½")]})
print(response)
```

æœ€ç»ˆç”Ÿæˆçš„å›¾ç»“æ„å¦‚ä¸‹ï¼š
![ä¸²è¡Œç®¡é“ç¤ºæ„å›¾](/img/sequential.png)

::: tip ğŸ“
å¯¹äºä¸²è¡Œç»„åˆçš„å›¾ï¼Œlanggraph çš„ StateGraph æä¾›äº† add_sequence æ–¹æ³•ä½œä¸ºç®€ä¾¿å†™æ³•ã€‚è¯¥æ–¹æ³•æœ€é€‚åˆåœ¨èŠ‚ç‚¹ä¸ºå‡½æ•°ï¼ˆè€Œéå­å›¾ï¼‰æ—¶ä½¿ç”¨ã€‚è‹¥èŠ‚ç‚¹ä¸ºå­å›¾ï¼Œä»£ç å¯èƒ½å¦‚ä¸‹ï¼š

```python
graph = StateGraph(AgentState)
graph.add_sequence([("graph1", graph1), ("graph2", graph2), ("graph3", graph3)])
graph.add_edge("__start__", "graph1")
graph = graph.compile()
```

ä¸è¿‡ï¼Œä¸Šè¿°å†™æ³•ä»æ˜¾ç¹çã€‚å› æ­¤ï¼Œæ›´æ¨èä½¿ç”¨ sequential_pipeline å‡½æ•°ï¼Œå®ƒèƒ½é€šè¿‡ä¸€è¡Œä»£ç å¿«é€Ÿæ„å»ºä¸²è¡Œæ‰§è¡Œå›¾ï¼Œæ›´ä¸ºç®€æ´é«˜æ•ˆã€‚
:::

## å¹¶è¡Œç®¡é“

å°†å¤šä¸ªçŠ¶æ€å›¾å¹¶è¡Œç»„åˆï¼Œæ”¯æŒçµæ´»çš„å¹¶è¡Œæ‰§è¡Œç­–ç•¥ã€‚

é€šè¿‡ä¸‹é¢å‡½æ•°å®ç°:

- `parallel_pipeline` - ä»¥å¹¶è¡Œæ–¹å¼ç»„åˆå¤šä¸ªçŠ¶æ€å›¾

æ”¯æŒçš„å‚æ•°å¦‚ä¸‹:

- `sub_graphs`: è¦ç»„åˆçš„çŠ¶æ€å›¾åˆ—è¡¨
- `state_schema`: æœ€ç»ˆç”Ÿæˆå›¾çš„ State Schema
- `branches_fn`: å¹¶è¡Œåˆ†æ”¯å‡½æ•°ï¼Œè¿”å› Send å¯¹è±¡åˆ—è¡¨æ§åˆ¶å¹¶è¡Œæ‰§è¡Œ
- `graph_name`: æœ€ç»ˆç”Ÿæˆå›¾çš„åç§°ï¼ˆå¯é€‰ï¼‰
- `context_schema`: æœ€ç»ˆç”Ÿæˆå›¾çš„ Context Schemaï¼ˆå¯é€‰ï¼‰
- `input_schema`: æœ€ç»ˆç”Ÿæˆå›¾çš„è¾“å…¥ Schemaï¼ˆå¯é€‰ï¼‰
- `output_schema`: æœ€ç»ˆç”Ÿæˆå›¾çš„è¾“å‡º Schemaï¼ˆå¯é€‰ï¼‰
- `checkpoint`: LangGraph çš„æŒä¹…åŒ– Checkpointï¼ˆå¯é€‰ï¼‰
- `store`: LangGraph çš„æŒä¹…åŒ– Storeï¼ˆå¯é€‰ï¼‰
- `cache`: LangGraph çš„ Cacheï¼ˆå¯é€‰ï¼‰

ä½¿ç”¨ç¤ºä¾‹:

**åŸºç¡€å¹¶è¡Œç¤ºä¾‹**

```python
from langchain_dev_utils.pipeline import parallel_pipeline

# æ„å»ºå¹¶è¡Œç®¡é“ï¼ˆæ‰€æœ‰å­å›¾å¹¶è¡Œæ‰§è¡Œï¼‰
graph = parallel_pipeline(
    sub_graphs=[
        create_agent(
            model="vllm:qwen3-4b",
            tools=[get_current_time],
            system_prompt="ä½ æ˜¯ä¸€ä¸ªæ—¶é—´æŸ¥è¯¢åŠ©æ‰‹,ä»…èƒ½å›ç­”å½“å‰æ—¶é—´,å¦‚æœè¿™ä¸ªé—®é¢˜å’Œæ—¶é—´æ— å…³,è¯·ç›´æ¥å›ç­”æˆ‘æ— æ³•å›ç­”",
            name="time_agent",
        ),
        create_agent(
            model="vllm:qwen3-4b",
            tools=[get_current_weather],
            system_prompt="ä½ æ˜¯ä¸€ä¸ªå¤©æ°”æŸ¥è¯¢åŠ©æ‰‹,ä»…èƒ½å›ç­”å½“å‰å¤©æ°”,å¦‚æœè¿™ä¸ªé—®é¢˜å’Œå¤©æ°”æ— å…³,è¯·ç›´æ¥å›ç­”æˆ‘æ— æ³•å›ç­”",
            name="weather_agent",
        ),
        create_agent(
            model="vllm:qwen3-4b",
            tools=[get_current_user],
            system_prompt="ä½ æ˜¯ä¸€ä¸ªç”¨æˆ·æŸ¥è¯¢åŠ©æ‰‹,ä»…èƒ½å›ç­”å½“å‰ç”¨æˆ·,å¦‚æœè¿™ä¸ªé—®é¢˜å’Œç”¨æˆ·æ— å…³,è¯·ç›´æ¥å›ç­”æˆ‘æ— æ³•å›ç­”",
            name="user_agent",
        ),
    ],
    state_schema=AgentState,
)
response = graph.invoke({"messages": [HumanMessage("ä½ å¥½")]})
print(response)
```

æœ€ç»ˆç”Ÿæˆçš„å›¾ç»“æ„å¦‚ä¸‹ï¼š
![å¹¶è¡Œç®¡é“ç¤ºæ„å›¾](/img/parallel.png)

**ä½¿ç”¨åˆ†æ”¯å‡½æ•°æ§åˆ¶å¹¶è¡Œæ‰§è¡Œ**

æœ‰äº›æ—¶å€™éœ€è¦æ ¹æ®æ¡ä»¶æŒ‡å®šå¹¶è¡Œæ‰§è¡Œå“ªäº›å­å›¾ï¼Œè¿™æ—¶å¯ä»¥ä½¿ç”¨åˆ†æ”¯å‡½æ•°ã€‚
åˆ†æ”¯å‡½æ•°éœ€è¦è¿”å›`Send`åˆ—è¡¨ã€‚

```python
# æ„å»ºå¹¶è¡Œç®¡é“ï¼ˆæ‰€æœ‰å­å›¾å¹¶è¡Œæ‰§è¡Œï¼‰
from langgraph.types import Send

graph = parallel_pipeline(
    sub_graphs=[
        create_agent(
            model="vllm:qwen3-4b",
            tools=[get_current_time],
            system_prompt="ä½ æ˜¯ä¸€ä¸ªæ—¶é—´æŸ¥è¯¢åŠ©æ‰‹,ä»…èƒ½å›ç­”å½“å‰æ—¶é—´,å¦‚æœè¿™ä¸ªé—®é¢˜å’Œæ—¶é—´æ— å…³,è¯·ç›´æ¥å›ç­”æˆ‘æ— æ³•å›ç­”",
            name="time_agent",
        ),
        create_agent(
            model="vllm:qwen3-4b",
            tools=[get_current_weather],
            system_prompt="ä½ æ˜¯ä¸€ä¸ªå¤©æ°”æŸ¥è¯¢åŠ©æ‰‹,ä»…èƒ½å›ç­”å½“å‰å¤©æ°”,å¦‚æœè¿™ä¸ªé—®é¢˜å’Œå¤©æ°”æ— å…³,è¯·ç›´æ¥å›ç­”æˆ‘æ— æ³•å›ç­”",
            name="weather_agent",
        ),
        create_agent(
            model="vllm:qwen3-4b",
            tools=[get_current_user],
            system_prompt="ä½ æ˜¯ä¸€ä¸ªç”¨æˆ·æŸ¥è¯¢åŠ©æ‰‹,ä»…èƒ½å›ç­”å½“å‰ç”¨æˆ·,å¦‚æœè¿™ä¸ªé—®é¢˜å’Œç”¨æˆ·æ— å…³,è¯·ç›´æ¥å›ç­”æˆ‘æ— æ³•å›ç­”",
            name="user_agent",
        ),
    ],
    state_schema=AgentState,
    branches_fn=lambda e: [
        Send("weather_agent", arg={"messages": [HumanMessage("è·å–å½“å‰New Yorkå¤©æ°”")]}),
        Send("time_agent", arg={"messages": [HumanMessage("è·å–å½“å‰æ—¶é—´")]}),
    ],
)


response = graph.invoke({"messages": [HumanMessage("ä½ å¥½")]})
print(response)
```

é‡è¦æ³¨æ„äº‹é¡¹

- ä¸ä¼ å…¥ `branches_fn` å‚æ•°æ—¶ï¼Œæ‰€æœ‰å­å›¾éƒ½ä¼šå¹¶è¡Œæ‰§è¡Œã€‚
- ä¼ å…¥ `branches_fn` å‚æ•°æ—¶ï¼Œæ‰§è¡Œå“ªäº›å­å›¾ç”±è¯¥å‡½æ•°çš„è¿”å›å€¼å†³å®šã€‚
