# å¯¹è¯æ¨¡å‹ç®¡ç†

> [!NOTE]
>
> **åŠŸèƒ½æ¦‚è¿°**ï¼šæä¾›æ›´é«˜æ•ˆã€æ›´ä¾¿æ·çš„å¯¹è¯æ¨¡å‹ç®¡ç†ã€‚
>
> **å‰ç½®è¦æ±‚**ï¼šäº†è§£ langchain çš„[å¯¹è¯æ¨¡å‹](https://docs.langchain.com/oss/python/langchain/models)ã€‚
>
> **é¢„è®¡é˜…è¯»æ—¶é—´**ï¼š10 åˆ†é’Ÿ

åœ¨ `langchain` ä¸­ï¼Œ`init_chat_model` å‡½æ•°å¯ç”¨äºåˆå§‹åŒ–å¯¹è¯æ¨¡å‹å®ä¾‹ï¼Œä½†å…¶æ”¯æŒçš„æ¨¡å‹æä¾›å•†è¾ƒä¸ºæœ‰é™ã€‚å¦‚æœä½ å¸Œæœ›ä½¿ç”¨æ›´å¤šæ¨¡å‹æä¾›å•†ï¼ˆå°¤å…¶æ˜¯ä½ åå¥½çš„æä¾›å•†æœªè¢«è¯¥å‡½æ•°æ”¯æŒï¼‰ï¼Œå¯ä»¥å€ŸåŠ©æœ¬åº“æä¾›çš„å¯¹è¯æ¨¡å‹ç®¡ç†åŠŸèƒ½æ¥å®ç°ã€‚

ä½¿ç”¨å¯¹è¯æ¨¡å‹æ—¶ï¼Œéœ€è¦å…ˆä½¿ç”¨`register_model_provider`æ³¨å†Œå¯¹è¯æ¨¡å‹æä¾›å•†ï¼Œç„¶åæ‰èƒ½ä½¿ç”¨`load_chat_model`åŠ è½½å¯¹è¯æ¨¡å‹ã€‚

## æ³¨å†Œå¯¹è¯æ¨¡å‹æä¾›å•†

æ³¨å†Œå¯¹è¯æ¨¡å‹æä¾›å•†éœ€è¦ä½¿ç”¨å‡½æ•°`register_model_provider`ã€‚å¯¹äºè¿™ä¸ªå‡½æ•°ï¼Œå®ƒæ¥æ”¶ä»¥ä¸‹å‚æ•°ï¼š

<Params
name="provider_name"
type="string"
description="å¯¹è¯æ¨¡å‹æä¾›å•†åç§°"
:required="true"
:default="null"
/>
<Params
name="chat_model"
type="BaseChatModel | string"
description="å¯¹è¯æ¨¡å‹"
:required="true"
:default="null"
/>
<Params
name="base_url"
type="string"
description="å¯¹è¯æ¨¡å‹åŸºç¡€ URL"
:required="false"
:default="null"
/>
<Params
name="provider_config"
type="dict"
description="å¯¹è¯æ¨¡å‹æä¾›å•†ç›¸å…³é…ç½®"
:required="false"
:default="null"
/>

å¯¹äºä¸Šè¿°å‚æ•°çš„å…·ä½“ä½¿ç”¨æ–¹æ³•å¦‚ä¸‹ï¼š

<StepItem step="1" title="è®¾ç½® provider_name"></StepItem>
é¦–å…ˆéœ€è¦ä¼ å…¥ `provider_name`å‚æ•°ä»¥æŒ‡å®šæ¨¡å‹æä¾›å•†ã€‚æ­¤åç§°å¯è‡ªå®šä¹‰ï¼Œå»ºè®®ä½¿ç”¨å…·æœ‰æ˜ç¡®å«ä¹‰çš„åç§°ï¼ˆå¦‚`vllm`ï¼‰æ¥æŒ‡ä»£çœŸå®çš„æä¾›å•†ã€‚è¯·æ³¨æ„ï¼Œåç§°ä¸­è¯·å‹¿åŒ…å«å†’å·`:`ï¼Œå› ä¸ºè¯¥ç¬¦å·åç»­å°†ç”¨äºåˆ†éš”æä¾›å•†ä¸æ¨¡å‹åç§°ã€‚

<StepItem step="2" title="è®¾ç½® chat_model"></StepItem>

æ¥ä¸‹æ¥éœ€è¦ä¼ å…¥ `chat_model`å‚æ•°ï¼Œè¿™ä¸ªå‚æ•°æ¥æ”¶ä¸¤ç§ç±»å‹ï¼š`langchain` çš„ `ChatModel` æˆ–è€… `str`ã€‚

å¯¹äºè¿™ä¸ªå‚æ•°çš„ä¸åŒç±»å‹ï¼Œæˆ‘ä»¬åˆ†åˆ«ä»‹ç»ï¼š

**1.ç±»å‹ä¸º ChatModel**

ç¤ºä¾‹ä»£ç å¦‚ä¸‹ï¼š

```python
from langchain_core.language_models.fake_chat_models import FakeChatModel
from langchain_dev_utils.chat_models import load_chat_model, register_model_provider

register_model_provider(
    provider_name="fake_provider",
    chat_model=FakeChatModel,
)
```

åœ¨æœ¬ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯ `langchain_core` å†…ç½®çš„ `FakeChatModel`ï¼Œå®ƒä»…ç”¨äºæµ‹è¯•ï¼Œå¹¶ä¸å¯¹æ¥çœŸå®çš„æ¨¡å‹æä¾›å•†ã€‚**ä½†åœ¨å®é™…åº”ç”¨ä¸­ï¼Œåº”ä¼ å…¥ä¸€ä¸ªå…·æœ‰å®é™…åŠŸèƒ½çš„ `ChatModel` ç±»ã€‚**

**2.ç±»å‹ä¸º str**

å½“ `chat_model` å‚æ•°ä¸ºå­—ç¬¦ä¸²æ—¶ï¼Œå…¶ç›®å‰å”¯ä¸€å–å€¼ä¸º `"openai-compatible"`ï¼Œè¡¨ç¤ºå°†é€šè¿‡æ¨¡å‹æä¾›å•†çš„ **OpenAI å…¼å®¹ API** è¿›è¡Œæ¥å…¥ã€‚å› ä¸ºç›®å‰å¾ˆå¤šæ¨¡å‹æä¾›å•†éƒ½æ”¯æŒ OpenAI å…¼å®¹ APIï¼Œä¾‹å¦‚[vLLM](https://github.com/vllm-project/vllm)ã€[OpenRouter](https://openrouter.ai/)ã€[Together AI](https://together.ai/) ç­‰ã€‚
æ­¤æ—¶ï¼Œæœ¬åº“ä¼šä½¿ç”¨å†…ç½®çš„ `BaseChatOpenAICompatible` ä½œä¸ºæ¨¡æ¿ç±»æ¥åˆ›å»ºå®é™…çš„å¯¹è¯æ¨¡å‹ç±»ã€‚

`BaseChatOpenAICompatible` ç»§æ‰¿è‡ª `langchain-openai` ä¸­çš„ `BaseChatOpenAI`ï¼Œå¹¶åœ¨å…¶åŸºç¡€ä¸Šè¿›è¡Œäº†å¤šé¡¹å…¼å®¹æ€§ä¼˜åŒ–ã€‚ä¸ºç¡®ä¿åŠŸèƒ½æ­£å¸¸ï¼Œè¯·åŠ¡å¿…å®‰è£…æ ‡å‡†ç‰ˆçš„ `langchain-dev-utils`ï¼ˆå®‰è£…æ–¹æ³•è¯¦è§ [å®‰è£…æ–‡æ¡£](../installation.md)ï¼‰ã€‚

ç›¸è¾ƒäºç›´æ¥ä½¿ç”¨ `langchain-openai` æä¾›çš„ `ChatOpenAI`ï¼Œæœ¬åº“çš„ `BaseChatOpenAICompatible` å…·æœ‰ä»¥ä¸‹ä¼˜åŠ¿ï¼š

1. **æ”¯æŒè¾“å‡ºæ›´å¤šç±»å‹çš„æ€ç»´é“¾å†…å®¹ï¼ˆ`reasoning_content`ï¼‰**ï¼š  
   `ChatOpenAI` ä»…èƒ½è¾“å‡ºå®˜æ–¹ OpenAI æ¨¡å‹åŸç”Ÿæ”¯æŒçš„æ€ç»´é“¾å†…å®¹ï¼Œè€Œ `OpenAICompatibleChatModel` å¯è¾“å‡ºå…¶å®ƒæ¨¡å‹æä¾›å•†çš„æ€ç»´é“¾å†…å®¹ï¼ˆä¾‹å¦‚ openrouter ç­‰ï¼‰ã€‚

2. **ä¼˜åŒ–ç»“æ„åŒ–è¾“å‡ºçš„é»˜è®¤è¡Œä¸º**ï¼š  
   åœ¨è°ƒç”¨ `with_structured_output` æ—¶ï¼Œ`method` å‚æ•°çš„é»˜è®¤å€¼è¢«è°ƒæ•´ä¸º `"function_calling"`ï¼ˆè€Œé `ChatOpenAI` é»˜è®¤çš„ `"json_schema"`ï¼‰ï¼Œä»è€Œæ›´å¥½åœ°å…¼å®¹å…¶å®ƒæ¨¡å‹ã€‚

3. **æ”¯æŒé…ç½®ç›¸å…³é…ç½®å‚æ•°**ï¼š  
   é’ˆå¯¹ä¸ OpenAI å®˜æ–¹ API éƒ¨åˆ†å‚æ•°å­˜åœ¨å·®å¼‚çš„æƒ…å†µï¼Œæœ¬åº“æä¾›äº† `provider_config` å‚æ•°ç”¨äºè§£å†³æœ¬é—®é¢˜ï¼Œä¾‹å¦‚ï¼Œä¸åŒæ¨¡å‹æä¾›å•†å¯¹ `tool_choice` çš„æ”¯æŒæ–¹å¼ä¸ä¸€è‡´æ—¶ï¼Œå¯é€šè¿‡åœ¨ `provider_config` ä¸­è®¾ç½® `supported_tool_choice` è¿›è¡Œé€‚é…ã€‚

<StepItem step="3" title="è®¾ç½® base_urlï¼ˆå¯é€‰ï¼‰"></StepItem>

æ¥ä¸‹æ¥ï¼Œéœ€è¦æ ¹æ®**å®é™…æƒ…å†µ**å†³å®šæ˜¯å¦è®¾ç½®æ¨¡å‹æä¾›å•†çš„ API åœ°å€ï¼ˆå³`base_url`å‚æ•°ï¼‰ã€‚è¯¥æ­¥éª¤**å¹¶éæ€»æ˜¯å¿…éœ€**ï¼Œå…·ä½“å–å†³äº `chat_model` çš„ç±»å‹ï¼š

- **å½“ `chat_model` ä¸ºå­—ç¬¦ä¸²ä¸”å€¼ä¸º `"openai-compatible"` æ—¶**ï¼š  
  å¿…é¡»æ˜¾å¼æä¾› `base_url` å‚æ•°ï¼Œæˆ–é€šè¿‡ç¯å¢ƒå˜é‡æŒ‡å®š API åœ°å€ã€‚å¦åˆ™æ¨¡å‹å®¢æˆ·ç«¯æ— æ³•åˆå§‹åŒ–ï¼Œå› ä¸ºç³»ç»Ÿæ— æ³•æ¨æ–­ API ç«¯ç‚¹ã€‚

- **å½“ `chat_model` ä¸º `ChatModel` ç±»å‹æ—¶**ï¼š  
   æ¨¡å‹çš„ API åœ°å€é€šå¸¸å·²åœ¨ç±»ä¸­å®šä¹‰ï¼Œæ— éœ€é¢å¤–é…ç½® `base_url`ã€‚  
   **ä»…å½“ä½ éœ€è¦è¦†ç›–è¯¥ç±»å†…ç½®çš„é»˜è®¤ API åœ°å€æ—¶**ï¼Œæ‰éœ€æ˜¾å¼ä¼ å…¥ `base_url` å‚æ•°æˆ–é€šè¿‡ç¯å¢ƒå˜é‡è®¾ç½®ï¼›æ­¤è¦†ç›–ä»…å¯¹ç±»ä¸­å­—æ®µåä¸º `api_base` æˆ– `base_url` çš„å±æ€§ç”Ÿæ•ˆï¼ˆåŒ…æ‹¬å­—æ®µåˆ«å alias ä¸ºè¿™ä¸¤ä¸ªåç§°çš„æƒ…å†µï¼‰ã€‚

ä¾‹å¦‚ï¼Œå‡è®¾æˆ‘ä»¬è¦ä½¿ç”¨ vLLM éƒ¨ç½²çš„ OpenAI å…¼å®¹æ¨¡å‹ï¼Œé‚£ä¹ˆå¯ä»¥è¿™æ ·è®¾ç½®ï¼š

**æ–¹æ³•ä¸€ï¼šç›´æ¥ä¼ å…¥ `base_url`**

```python
from langchain_dev_utils.chat_models import register_model_provider

register_model_provider(
    provider_name="vllm",
    chat_model="openai-compatible",
    base_url="http://localhost:8000/v1",
)
```

**æ–¹æ³•äºŒï¼šé€šè¿‡ç¯å¢ƒå˜é‡é…ç½®**

```bash
export VLLM_API_BASE=http://localhost:8000/v1
```

ç„¶ååœ¨ä»£ç ä¸­çœç•¥ `base_url`ï¼š

```python
from langchain_dev_utils.chat_models import register_model_provider

register_model_provider(
    provider_name="vllm",
    chat_model="openai-compatible"
    # è‡ªåŠ¨è¯»å– VLLM_API_BASE ç¯å¢ƒå˜é‡
)
```

> ğŸ’¡ æç¤ºï¼šç¯å¢ƒå˜é‡å‘½åè§„åˆ™ä¸º `${PROVIDER_NAME}_API_BASE`ï¼ˆå…¨å¤§å†™ï¼Œä¸‹åˆ’çº¿åˆ†éš”ï¼‰ã€‚

::: tip è¡¥å……
`vllm`æ˜¯çŸ¥åçš„å¤§æ¨¡å‹éƒ¨ç½²æ¡†æ¶ï¼Œå…¶å¯ä»¥å°†å¤§æ¨¡å‹éƒ¨ç½²ä¸º OpenAI å…¼å®¹ APIã€‚ä¾‹å¦‚æˆ‘ä»¬è¦éƒ¨ç½² qwen3-4b æ¨¡å‹ã€‚åˆ™å¯ä»¥ä½¿ç”¨å¦‚ä¸‹æŒ‡ä»¤ï¼š

```bash
vllm serve Qwen/Qwen3-4B \
--reasoning-parser qwen3 \
--enable-auto-tool-choice --tool-call-parser hermes \
--host 0.0.0.0 --port 8000 \
--served-model-name qwen3-4b
```

å®Œæˆåä¼šæä¾›ä¸€ä¸ª OpenAI å…¼å®¹ APIï¼Œåœ°å€ä¸º`http://localhost:8000/v1`ã€‚
:::

<StepItem step="4" title="è®¾ç½® provider_configï¼ˆå¯é€‰ï¼‰"></StepItem>

ä»…å½“ `chat_model` ä¸ºå­—ç¬¦ä¸² `"openai-compatible"` æ—¶ï¼Œæ­¤å‚æ•°æ‰æœ‰æ•ˆã€‚ç”¨äºé…ç½®æ¨¡å‹æä¾›å•†çš„ç›¸å…³å‚æ•°ã€‚
ç›®å‰æ”¯æŒé…ç½®ä»¥ä¸‹æä¾›å•†å‚æ•°ï¼š

- `supported_tool_choice`ï¼šæ”¯æŒçš„ `tool_choice` å–å€¼ã€‚
- `keep_reasoning_content`ï¼šæ˜¯å¦åœ¨ä¼ ç»™æ¨¡å‹çš„å†å²ä¸Šä¸‹æ–‡æ¶ˆæ¯ä¸­ä¿ç•™æ¨ç†å†…å®¹ï¼ˆ`reasoning_content`ï¼‰ã€‚
- `support_json_mode`ï¼šæ˜¯å¦æ”¯æŒ `json_mode`çš„ç»“æ„åŒ–è¾“å‡ºæ–¹å¼ã€‚

**1. supported_tool_choice**

`tool_choice`æ˜¯å¤§éƒ¨åˆ†æ¨¡å‹æä¾›å•† API ç«¯ç‚¹å¸¸è§çš„å‚æ•°ã€‚è¯¥å‚æ•°é€šå¸¸æ¥å—ä»¥ä¸‹å–å€¼ï¼š

- `auto`ï¼šç”±æ¨¡å‹è‡ªä¸»å†³å®šæ˜¯å¦è°ƒç”¨å·¥å…·ï¼ˆå¤§å¤šæ•°æä¾›å•†çš„é»˜è®¤è¡Œä¸ºï¼‰ï¼›
- `none`ï¼šç¦æ­¢è°ƒç”¨ä»»ä½•å·¥å…·ï¼›
- `required`ï¼šå¼ºåˆ¶æ¨¡å‹å¿…é¡»è°ƒç”¨è‡³å°‘ä¸€ä¸ªå·¥å…·ï¼›
- `æŒ‡å®šæŸä¸€ç‰¹å®šå·¥å…·`ï¼šå¼ºåˆ¶è°ƒç”¨æŒ‡å®šåç§°çš„å·¥å…·ï¼ˆä¸€èˆ¬éœ€è¦ä¼ é€’å…·ä½“çš„å·¥å…·çš„åç§°ï¼Œä¾‹å¦‚åœ¨ OpenAI API ä¸­ï¼Œéœ€è¦ä¼ é€’ `tool_choice={"type": "function", "function": {"name": "get_weather"}}`ï¼‰ã€‚

ç„¶è€Œï¼Œä¸åŒæ¨¡å‹æä¾›å•†å¯¹ `tool_choice` çš„æ”¯æŒèŒƒå›´å¹¶ä¸ä¸€è‡´ã€‚æœ‰äº›æ”¯æŒä¸Šè¿°çš„å¤§éƒ¨åˆ†å–å€¼ï¼Œæœ‰äº›ä»…æ”¯æŒæœ€åŸºç¡€çš„`auto`å–å€¼ã€‚
ä½†æ˜¯ï¼ŒæŸäº›é«˜å±‚å°è£…åº“ä¸ºäº†æé«˜å…¶ç¨³å®šæ€§ï¼Œä¼šä¼ é€’æŸä¸ªç‰¹å®šçš„`tool_choice`å‚æ•°ï¼Œæ­¤æ—¶å¦‚æœå¯¹æ¥çš„æ¨¡å‹æä¾›å•† API ä¸æ”¯æŒï¼Œåˆ™è°ƒç”¨ä¼šå¼•å‘å¼‚å¸¸ã€‚ä¸ºäº†è§£å†³ä¸Šè¿°ç°è±¡ï¼Œæœ¬åº“é»˜è®¤çš„åšæ³•æ˜¯è¿‡æ»¤ä»»ä½•çš„`tool_choice`å‚æ•°å–å€¼ï¼Œä½¿å¾—æœ€ç»ˆä¼ é€’ç»™å¤§æ¨¡å‹ API ä¸ä¼šæœ‰`tool_choice`å‚æ•°ï¼ˆå“ªæ€•ä½¿ç”¨è€…æ˜¾å¼åœ°ä¼ é€’äº†`tool_choice`å‚æ•°ï¼‰ï¼Œè¿™æ ·åšèƒ½å¤Ÿå°½å¯èƒ½çš„é¿å…å…¼å®¹æ€§é—®é¢˜å¼•å‘çš„é”™è¯¯ã€‚

ä½†æ˜¯ï¼Œæœ‰äº›æ—¶å€™ç¡®å®éœ€è¦é€šè¿‡è®¾ç½®`tool_choice`æ¥æå‡åº”ç”¨ç¨³å®šæ€§ã€‚ä¾‹å¦‚åœ¨ç»“æ„åŒ–è¾“å‡ºçš„åœºæ™¯ä¸‹ï¼Œç”±äºæç¤ºè¯é—®é¢˜æˆ–è€…æ¨¡å‹æ€§èƒ½é—®é¢˜å¯èƒ½ä¼šè¾“å‡º Noneï¼Œæ­¤æ—¶å¦‚æœæ¨¡å‹æä¾›å•†æ”¯æŒè®¾ç½®æŒ‡å®šæŸä¸€ç‰¹å®šå·¥å…·çš„ç­–ç•¥æ–¹å¼ï¼Œé‚£ä¹ˆå¯ä»¥ä½¿ç”¨`tool_choice`æ¥æé«˜ç»“æ„åŒ–è¾“å‡ºçš„æ­£ç¡®æ€§ã€‚

å¯¹æ­¤ï¼Œæœ¬åº“å¼•å…¥äº†æœ¬é…ç½®é¡¹`supported_tool_choice`ï¼Œè¯¥é…ç½®é¡¹æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²åˆ—è¡¨ï¼Œé»˜è®¤æƒ…å†µä¸‹æ˜¯ä¸€ä¸ªç©ºåˆ—è¡¨ï¼Œå³è¿‡æ»¤æ‰€æœ‰çš„`tool_choice`å‚æ•°å–å€¼ã€‚æ¯ä¸ªå­—ç¬¦ä¸²çš„å–å€¼åªèƒ½æ˜¯`auto`ã€`none`ã€`required`ã€`specific`ã€‚å…¶ä¸­ï¼Œå‰ä¸‰é¡¹å¯¹åº”æ ‡å‡†çš„ `tool_choice` ç­–ç•¥ï¼Œè€Œ`specific` æ˜¯æœ¬åº“ç‰¹æœ‰çš„æ ‡è¯†ï¼Œè¡¨ç¤ºæœ€åä¸€ç§ç­–ç•¥ï¼Œå³æŒ‡å®šæŸä¸€ç‰¹å®šå·¥å…·ã€‚

ä¾‹å¦‚ï¼š

vLLM æ”¯æŒ `auto`ã€`none`ã€`required` ä»¥åŠæŒ‡å®šç‰¹å®šå·¥å…·çš„ `tool_choice`ï¼ˆå³å…¨éƒ¨çš„å¯èƒ½çš„`tool_choice`å–å€¼ï¼‰ã€‚å› æ­¤ï¼Œåœ¨æœ¬åº“ä¸­åº”å°†è¯¥å‚æ•°è®¾ä¸ºï¼š

```python
from langchain_dev_utils.chat_models import register_model_provider

register_model_provider(
    provider_name="vllm",
    chat_model="openai-compatible",
    provider_config={"supported_tool_choice": ["auto", "none", "required", "specific"]},
)
```

**2. support_json_mode**

ç»“æ„åŒ–è¾“å‡ºçš„å®ç°ä¸»è¦ä¾èµ–ä¸¤ç§æ–¹å¼ï¼š`function_calling` ä¸ `json_mode`ã€‚å…¶ä¸­ï¼Œ`function_calling` æ˜¯å½“å‰æœ€å¸¸ç”¨çš„æ–¹å¼ï¼Œé€‚ç”¨äºç»å¤§å¤šæ•°å¤§æ¨¡å‹ APIï¼›è€Œ `json_mode` æ˜¯éƒ¨åˆ†æ¨¡å‹æä¾›å•†æä¾›çš„ä¸€ç§ç»“æ„åŒ–è¾“å‡ºæ–¹å¼ï¼Œå…è®¸æ¨¡å‹ç›´æ¥ç”Ÿæˆç¬¦åˆæŒ‡å®š JSON Schema çš„ç»“æ„åŒ–å“åº”ï¼Œæ— éœ€é¢„å…ˆå®šä¹‰å·¥å…·å‡½æ•°ï¼Œä»è€Œç®€åŒ–è°ƒç”¨æµç¨‹å¹¶æå‡è¾“å‡ºä¸€è‡´æ€§ã€‚

åœ¨ OpenAI API ä¸­ï¼Œå¯ç”¨ `json_mode` éœ€æ˜¾å¼è®¾ç½® `response_format={"type": "json_object"}`ã€‚

è‹¥ä½ çš„æ¨¡å‹æä¾›å•†æ”¯æŒ `json_mode`ï¼Œå¯å°†`support_json_mode`å‚æ•°è®¾ä¸º `True`ï¼Œå¹¶åœ¨è°ƒç”¨ `with_structured_output` æ—¶æ˜¾å¼æŒ‡å®š `method="json_mode"` ä»¥å¯ç”¨è¯¥æ¨¡å¼ã€‚

**3. keep_reasoning_content**

è¿™ä¸ªå‚æ•°çš„ä½œç”¨æ˜¯ç”¨äºæ§åˆ¶æœ€ç»ˆä¼ ç»™æ¨¡å‹çš„ä¸Šä¸‹æ–‡å†å²è®°å½•ï¼ˆ`messages`ï¼‰ä¸­è¦ä¸è¦ä¿ç•™æ¨¡å‹æ¨ç†å†…å®¹ï¼ˆ`reasoning content`ï¼‰ã€‚é»˜è®¤æ˜¯ `False`ï¼Œå³ä¸ä¿ç•™ï¼Œè¿™ä¹Ÿæ˜¯å¤§éƒ¨åˆ†æ¨¡å‹æä¾›å•†è¦æ±‚çš„æ–¹å¼ï¼Œå³æœ€ç»ˆä¼ ç»™å¤§æ¨¡å‹çš„ä¸Šä¸‹æ–‡å†…å®¹ä¸åº”è¯¥åŒ…å«æ¨ç†å†…å®¹ã€‚å¦‚ä¸‹æ‰€ç¤ºï¼Œè¿™æ˜¯å¤§å¤šæ•°æä¾›å•†çš„ `messages` æ ¼å¼ï¼Œä¸åŒ…å«æ¨ç†å†…å®¹ï¼š

```json
[
  { "role": "user", "content": "ä½ å¥½" },
  { "role": "assistant", "content": "ä½ å¥½ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼Ÿ" }
]
```

ä½†æ˜¯æœ‰äº›æä¾›å•†ä¹Ÿä¼šå»ºè®®ä¿ç•™æ¨ç†å†…å®¹è¿›ä¸€æ­¥æé«˜æ¨ç†èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨é¢å¯¹å¤šæ­¥å·¥å…·è°ƒç”¨çš„å¤æ‚ä»»åŠ¡åœºæ™¯ã€‚**æœ¬å‚æ•°çš„ä½œç”¨å°±æ˜¯ä¸ºäº†åº”å¯¹è¿™ä¸ªæƒ…å†µã€‚** å½“å‚æ•°ä¸º `True`ï¼ˆä¿ç•™æ¨ç†å†…å®¹ï¼‰æ—¶ï¼Œä¼ å…¥æ¨¡å‹çš„ `messages` å¦‚ä¸‹ï¼š

```json
[
  { "role": "user", "content": "ä½ å¥½" },
  {
    "role": "assistant",
    "content": "ä½ å¥½ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼Ÿ",
    "reasoning_content": "ç”¨æˆ·è¯´äº†â€˜ä½ å¥½â€™ï¼Œè¿™æ˜¯æ‰“æ‹›å‘¼ï¼Œæˆ‘åº”è¯¥ç¤¼è²Œå›åº”å¹¶ä¸»åŠ¨è¯¢é—®éœ€æ±‚ã€‚"
  }
]
```

å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå¤§éƒ¨åˆ†æƒ…å†µä¸‹ä½ ä¸åº”è¯¥ä¸»åŠ¨è®¾ç½®è¿™ä¸ªå‚æ•°ä¸º `true`ï¼Œå› ä¸ºä¼ å…¥æ¨ç†å†…å®¹ä¼šå¯¼è‡´ä¸Šä¸‹æ–‡æ¶ˆè€—å¢å¤šï¼Œæœ‰äº›æä¾›å•†ç”šè‡³ä¸å…è®¸ä¼ å…¥æ¨ç†å†…å®¹ï¼ˆå¦‚ DeepSeekï¼‰ã€‚é™¤éæ¨¡å‹æä¾›å•†æ–‡æ¡£ä¸­æ˜ç¡®å»ºè®®ä¼ å…¥ï¼Œæ­¤æ—¶ä½ å¯ä»¥è€ƒè™‘ä¼ å…¥ã€‚

**æ³¨æ„**ï¼šä¸Šé¢ä¾‹å­æ¯”è¾ƒç®€å•ï¼Œå®é™…åœ¨æ™ºèƒ½ä½“åœºæ™¯ä¸‹éƒ¨åˆ† message å¯èƒ½ä¼šåŒ…å«å·¥å…·è°ƒç”¨ç­‰å†…å®¹ã€‚

## æ‰¹é‡æ³¨å†Œ

å¦‚æœä½ éœ€è¦æ³¨å†Œå¤šä¸ªæ¨¡å‹æä¾›å•†ï¼Œå¯ä»¥å¤šæ¬¡ä½¿ç”¨`register_model_provider`å‡½æ•°ã€‚ä½†æ˜¯è¿™æ ·æ˜¾ç„¶ç‰¹åˆ«éº»çƒ¦ï¼Œå› æ­¤æœ¬åº“æä¾›äº†ä¸€ä¸ªæ‰¹é‡æ³¨å†Œçš„å‡½æ•°`batch_register_model_provider`ã€‚

å…¶æ¥æ”¶çš„å‚æ•°æ˜¯ providersï¼Œå…¶ä¸ºä¸€ä¸ªå­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸æœ‰å››ä¸ªé”®åˆ†åˆ«æ˜¯`provider_name`ã€`chat_model`ã€`base_url`(å¯é€‰)ã€`provider_config`(å¯é€‰)ã€‚æ¯ä¸ªé”®çš„æ„ä¹‰ä¸`register_model_provider`å‡½æ•°ä¸­çš„å‚æ•°æ„ä¹‰ç›¸åŒã€‚

ç¤ºä¾‹ä»£ç å¦‚ä¸‹ï¼š

```python
from langchain_dev_utils.chat_models import (
    batch_register_model_provider,
    load_chat_model,
)
from langchain_core.language_models.fake_chat_models import FakeChatModel

batch_register_model_provider(
    providers=[
        {
            "provider_name": "fake_provider",
            "chat_model": FakeChatModel,
        },
        {
            "provider_name": "vllm",
            "chat_model": "openai-compatible",
            "base_url": "http://localhost:8000/v1",
        },
    ]
)

model = load_chat_model("vllm:qwen3-4b")
print(model.invoke("Hello"))

model = load_chat_model("fake_provider:fake-model")
print(model.invoke("Hello"))
```

::: warning æ³¨æ„
`register_model_provider` åŠå…¶å¯¹åº”çš„æ‰¹é‡æ³¨å†Œå‡½æ•° `batch_register_model_provider` å‡åŸºäºä¸€ä¸ªå…¨å±€å­—å…¸å®ç°ã€‚ä¸ºé¿å…å¤šçº¿ç¨‹å¹¶å‘é—®é¢˜ï¼Œè¯·åŠ¡å¿…åœ¨é¡¹ç›®å¯åŠ¨é˜¶æ®µå®Œæˆæ‰€æœ‰æ³¨å†Œæ“ä½œï¼Œåˆ‡å‹¿åœ¨è¿è¡Œæ—¶åŠ¨æ€æ³¨å†Œã€‚
:::

## åŠ è½½å¯¹è¯æ¨¡å‹

åŠ è½½å¯¹è¯æ¨¡å‹çš„å‡½æ•°æ˜¯`load_chat_model`ï¼Œå…¶æ¥æ”¶ä»¥ä¸‹å‚æ•°ï¼š

<Params
name="model"
type="string"
description="å¯¹è¯æ¨¡å‹åç§°"
:required="true"
:default="null"
/>
<Params
name="model_provider"
type="string"
description="å¯¹è¯æ¨¡å‹æä¾›å•†åç§°"
:required="false"
:default="null"
/>
åŒæ—¶å¯¹äºè¿™ä¸ªå‡½æ•°çš„ä½¿ç”¨è¿˜éœ€è¦æ³¨æ„ä»¥ä¸‹å†…å®¹ï¼š

**1.é¢å¤–å‚æ•°**

è¯¥å‡½æ•°è¿˜èƒ½æ¥æ”¶ä»»æ„æ•°é‡ä¸ªå…³é”®å­—å‚æ•°ï¼Œä¾‹å¦‚`temperature`ã€`max_tokens`ç­‰,å…·ä½“å‚è€ƒå¯¹åº”çš„æ¨¡å‹é›†æˆç±»æ–‡æ¡£ï¼ˆå¦‚æœ chat_model æ˜¯`openai-compatible`ï¼Œåˆ™å¯ä»¥å‚è€ƒ`ChatOpenAI`ï¼‰ã€‚

**2.model å‚æ•°æ ¼å¼**

`model` å‚æ•°æ”¯æŒä¸¤ç§æ ¼å¼ï¼š

1. `provider_name:model_name`
2. `model_name`

å…¶ä¸­ï¼Œ`provider_name` ä¸ºé€šè¿‡ `register_model_provider` å‡½æ•°æ³¨å†Œçš„æä¾›å•†åç§°ã€‚

`model_provider` å‚æ•°ä¸ä¸Šè¿° `provider_name` å«ä¹‰ç›¸åŒï¼Œä¸ºå¯é€‰å‚æ•°ï¼š

- è‹¥æœªä¼ å…¥ `model_provider`ï¼Œåˆ™ `model` å‚æ•°å¿…é¡»ä¸º `provider_name:model_name` æ ¼å¼ï¼›
- è‹¥ä¼ å…¥ `model_provider`ï¼Œåˆ™ `model` å‚æ•°å¿…é¡»ä¸º `model_name` æ ¼å¼ã€‚

ç¤ºä¾‹ä»£ç å¦‚ä¸‹ï¼š

```python
from langchain_dev_utils.chat_models import load_chat_model
from langchain_core.messages import HumanMessage

model = load_chat_model("vllm:qwen3-4b")
response = model.invoke([HumanMessage("Hello")])
print(response)
```

ä¹Ÿå¯ä»¥ç›´æ¥ä¼ å…¥`model_provider`å‚æ•°ã€‚

```python
from langchain_dev_utils.chat_models import load_chat_model

model = load_chat_model("qwen3-4b", model_provider="vllm")
```

**æ³¨æ„**ï¼šè™½ç„¶ vllm æœ¬èº«å¯ä»¥ä¸è¦æ±‚ api_key,ä½†æ˜¯ç”±äºè¿™ä¸ªå¯¹è¯æ¨¡å‹ç±»éœ€è¦ api_keyï¼Œå› æ­¤ä½ å¿…é¡»è®¾ç½® api_keyã€‚

```bash
export VLLM_API_KEY=vllm
```

**3.chat_model ä¸ºå­—ç¬¦ä¸²æƒ…å†µä¸‹çš„å¯¹è¯æ¨¡å‹ç±»ç‰¹ç‚¹**

å¯¹äºä¸Šé¢æåˆ°çš„`chat_model`ä¸ºå­—ç¬¦ä¸²ï¼ˆå³`"openai-compatible"`ï¼‰çš„æƒ…å†µï¼Œå…¶æ”¯æŒä»¥ä¸‹ç‰¹ç‚¹ä»¥åŠåŠŸèƒ½ï¼š

::: details æ™®é€šè°ƒç”¨
ä¾‹å¦‚ï¼š

```python
from langchain_dev_utils.chat_models import load_chat_model
from langchain_core.messages import HumanMessage

model = load_chat_model("vllm:qwen3-4b")
response = model.invoke([HumanMessage("Hello")])
print(response)
```

:::

::: details å¼‚æ­¥è°ƒç”¨

åŒæ ·æ”¯æŒå¼‚æ­¥è°ƒç”¨

```python
from langchain_dev_utils.chat_models import load_chat_model
from langchain_core.messages import HumanMessage

model = load_chat_model("vllm:qwen3-4b")
response = await model.ainvoke([HumanMessage("Hello")])
print(response)
```

:::

::: details æµå¼è¾“å‡º
ä¾‹å¦‚ï¼š

```python
from langchain_dev_utils.chat_models import load_chat_model
from langchain_core.messages import HumanMessage

model = load_chat_model("vllm:qwen3-4b")
for chunk in model.stream([HumanMessage("Hello")]):
    print(chunk)
```

:::

::: details å¼‚æ­¥æµå¼è¾“å‡º
åŒæ ·ä¹Ÿæ”¯æŒå¼‚æ­¥çš„æµå¼è°ƒç”¨

```python
from langchain_dev_utils.chat_models import load_chat_model
from langchain_core.messages import HumanMessage

model = load_chat_model("vllm:qwen3-4b")
async for chunk in model.astream([HumanMessage("Hello")]):
    print(chunk)
```

:::

::: details å·¥å…·è°ƒç”¨
æ³¨æ„ï¼šéœ€è¦ä¿è¯æ¨¡å‹æ”¯æŒå·¥å…·è°ƒç”¨

```python
from langchain_dev_utils.chat_models import load_chat_model
from langchain_core.messages import HumanMessage
from langchain_core.tools import tool
import datetime

@tool
def get_current_time() -> str:
    """è·å–å½“å‰æ—¶é—´æˆ³"""
    return str(datetime.datetime.now().timestamp())

model = load_chat_model("vllm:qwen3-4b").bind_tools([get_current_time])
response = model.invoke([HumanMessage("è·å–å½“å‰æ—¶é—´æˆ³")])
print(response)
```

:::

::: details ç»“æ„åŒ–è¾“å‡º
é»˜è®¤é‡‡ç”¨`function_calling`æ–¹æ³•ï¼Œå› æ­¤æ¨¡å‹éœ€è¦æ”¯æŒå·¥å…·è°ƒç”¨

```python
from langchain_dev_utils.chat_models import load_chat_model
from langchain_core.messages import HumanMessage
from langchain_core.tools import tool
from pydantic import BaseModel

class User(BaseModel):
    name: str
    age: int

model = load_chat_model("vllm:qwen3-4b").with_structured_output(User)
response = model.invoke([HumanMessage("ä½ å¥½ï¼Œæˆ‘å«å¼ ä¸‰ï¼Œä»Šå¹´25å²")])
print(response)
```

åŒæ—¶ï¼Œå¦‚æœä½ çš„æ¨¡å‹æä¾›å•†æ”¯æŒ`json_mode`ï¼Œåˆ™å¯åœ¨æ³¨å†Œæ¨¡å‹æä¾›å•†æ—¶ï¼Œå°†`provider_config`å‚æ•°ä¸­çš„`support_json_mode`è®¾ç½®ä¸º`True`ï¼Œå¹¶åœ¨è°ƒç”¨`with_structured_output`æ—¶å°†`method`å‚æ•°æŒ‡å®šä¸º`"json_mode"`ä»¥å¯ç”¨è¯¥æ¨¡å¼ã€‚æ­¤æ—¶ï¼Œå»ºè®®åœ¨æç¤ºè¯ä¸­æ˜ç¡®å¼•å¯¼æ¨¡å‹æŒ‰ç…§æŒ‡å®šçš„ JSON Schema æ ¼å¼è¾“å‡ºç»“æ„åŒ–æ•°æ®ã€‚
:::

::: details ä¼ é€’æ¨¡å‹å‚æ•°

é™¤æ­¤ä¹‹å¤–ï¼Œç”±äºè¯¥ç±»ç»§æ‰¿äº†`BaseChatOpenAI`,å› æ­¤æ”¯æŒä¼ é€’`BaseChatOpenAI`çš„æ¨¡å‹å‚æ•°ï¼Œä¾‹å¦‚`temperature`, `extra_body`ç­‰ã€‚
ç¤ºä¾‹ä»£ç å¦‚ä¸‹ï¼š

```python
from langchain_dev_utils.chat_models import load_chat_model
from langchain_core.messages import HumanMessage

model = load_chat_model("vllm:qwen3-4b",extra_body={"chat_template_kwargs": {"enable_thinking": False}}) #åˆ©ç”¨extra_bodyä¼ é€’é¢å¤–å‚æ•°ï¼Œè¿™é‡Œæ˜¯å…³é—­æ€è€ƒæ¨¡å¼
response = model.invoke([HumanMessage("Hello")])
print(response)
```

:::

::: details ä¼ é€’å¤šæ¨¡æ€æ•°æ®

å¦å¤–ï¼Œä¹Ÿæ”¯æŒä¼ é€’å¤šæ¨¡æ€æ•°æ®ï¼Œä½ å¯ä»¥ä½¿ç”¨ OpenAI å…¼å®¹çš„å¤šæ¨¡æ€æ•°æ®æ ¼å¼æˆ–è€…ç›´æ¥ä½¿ç”¨`langchain`ä¸­çš„`content_block`ã€‚ä¾‹å¦‚ï¼š

```python
from langchain_dev_utils.chat_models import register_model_provider, load_chat_model
from langchain_core.messages import HumanMessage


register_model_provider(
    provider_name="openrouter",
    chat_model="openai-compatible",
    base_url="https://openrouter.ai/api/v1",
)

messages = [
    HumanMessage(
        content_blocks=[
            {
                "type": "image",
                "url": "https://example.com/image.png",
            },
            {"type": "text", "text": "æè¿°è¿™å¼ å›¾ç‰‡"},
        ]
    )
]

model = load_chat_model("openrouter:qwen/qwen3-vl-8b-thinking")
response = model.invoke(messages)
print(response)
```

:::

::: details OpenAI æœ€æ–°çš„`responses_api`

æœ€åï¼Œè¿˜éœ€è¦å¼ºè°ƒä¸€ç‚¹ï¼Œè¯¥æ¨¡å‹ç±»ä¹Ÿæ”¯æŒ OpenAI æœ€æ–°çš„`responses_api`ã€‚ä½†æ˜¯ç›®å‰ä»…æœ‰å°‘é‡çš„æä¾›å•†æ”¯æŒè¯¥é£æ ¼çš„ APIã€‚å¦‚æœä½ çš„æ¨¡å‹æä¾›å•†æ”¯æŒè¯¥ API é£æ ¼ï¼Œåˆ™å¯ä»¥åœ¨ä¼ å…¥`use_responses_api`å‚æ•°ä¸º`True`ã€‚
ä¾‹å¦‚ vllm æ”¯æŒ`responses_api`ï¼Œåˆ™å¯ä»¥è¿™æ ·ä½¿ç”¨ï¼š

```python
from langchain_dev_utils.chat_models import load_chat_model
from langchain_core.messages import HumanMessage

model = load_chat_model("vllm:qwen3-4b", use_responses_api=True)
response = model.invoke([HumanMessage(content="ä½ å¥½")])
print(response)
```

:::

**4.ä¸å®˜æ–¹å‡½æ•°å…¼å®¹æƒ…å†µ**

å¯¹äºå®˜æ–¹ `init_chat_model` å‡½æ•°å·²æ”¯æŒçš„æ¨¡å‹æä¾›å•†ï¼Œä½ ä¹Ÿå¯ä»¥ç›´æ¥ä½¿ç”¨ `load_chat_model` å‡½æ•°è¿›è¡ŒåŠ è½½ï¼Œæ— éœ€é¢å¤–æ³¨å†Œã€‚å› æ­¤ï¼Œå¦‚æœä½ éœ€è¦åŒæ—¶æ¥å…¥å¤šä¸ªæ¨¡å‹ï¼Œå…¶ä¸­éƒ¨åˆ†æä¾›å•†ä¸ºå®˜æ–¹æ”¯æŒï¼Œå¦ä¸€éƒ¨åˆ†ä¸æ”¯æŒï¼Œå¯ä»¥è€ƒè™‘ç»Ÿä¸€ä½¿ç”¨ `load_chat_model` è¿›è¡ŒåŠ è½½ã€‚ä¾‹å¦‚ï¼š

```python
from langchain_dev_utils.chat_models import load_chat_model
from langchain_core.messages import HumanMessage

# åŠ è½½æ¨¡å‹æ—¶éœ€æŒ‡å®šæä¾›å•†ä¸æ¨¡å‹åç§°
model = load_chat_model("openai:gpt-4o-mini")
# æˆ–æ˜¾å¼æŒ‡å®šæä¾›å•†å‚æ•°
model = load_chat_model("openai:gpt-4o-mini", model_provider="openai")

# æ³¨æ„ï¼šå¿…é¡»æŒ‡å®šæ¨¡å‹æä¾›å•†ï¼Œæ— æ³•ä»…æ ¹æ®æ¨¡å‹åç§°è‡ªåŠ¨æ¨æ–­
response = model.invoke([HumanMessage("Hello")])
print(response)
```

<BestPractice>
    <p>å¯¹äºæœ¬æ¨¡å—çš„ä½¿ç”¨ï¼Œæœ‰å¦‚ä¸‹å»ºè®®ï¼š</p>
    <ol>
        <li>è‹¥æ‰€æœ‰æ¨¡å‹å‡è¢«å®˜æ–¹ <code>init_chat_model</code> æ”¯æŒï¼Œè¯·ç›´æ¥ä½¿ç”¨è¯¥å‡½æ•°ï¼Œä»¥è·å¾—æœ€ä½³å…¼å®¹æ€§å’Œç¨³å®šæ€§ã€‚</li>
        <li>è‹¥éƒ¨åˆ†æ¨¡å‹ä¸å—å®˜æ–¹æ”¯æŒï¼Œæˆ–éœ€è¦é›†æˆå®˜æ–¹æœªè¦†ç›–çš„æä¾›å•†ï¼Œå¯ä½¿ç”¨æœ¬æ¨¡å—çš„å‡½æ•°ã€‚</li>
        <li>å¦‚æœæš‚æ— é€‚åˆçš„æ¨¡å‹é›†æˆåº“ï¼Œä½†æä¾›å•†æä¾›äº† OpenAI å…¼å®¹çš„ APIï¼Œåˆ™å¯ä»¥ä½¿ç”¨æœ¬æ¨¡å—å‡½æ•°ã€‚</li>
    </ol>
</BestPractice>
